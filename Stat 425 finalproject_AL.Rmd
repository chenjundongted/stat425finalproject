---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---
***For Q3.1 & 3.2***

**Here we import the original file with changed column names. We see that there is no missing values. Then, we use the str and summary to get a quick overview of the dataset.**
```{r}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(stringr)
real = Real_estate_valuation_data_set_Rename_
names(real)
sum(is.null(real))
str(real)
summary(real)
```

**We will create a new column called month with input value derived from the transaction date column, and use that as the 7th predictor. However, we notice an inconsistency between the data description and the actual calculation.**
```{r}
real$Price <- real$Price*0.042*1/3/3*1000
real <- real[,-c(1)]
```
**According to the description, the unit used for unit area aren't used in U.S., so we decided to change it first for easier interpretation in the later steps. Also, since the number of the house is merely a number with no specific meaning, we will exclude that from the prediction model**
```{r}
Real_Estate = real %>%
  mutate(Trans_Month = round((Date - floor(Date)), digits = 3)) %>%
  mutate(Trans_Month = ifelse(Trans_Month == 0.000, "January",
                       ifelse(Trans_Month == 0.083, "February",
                       ifelse(Trans_Month == 0.167, "March",
                       ifelse(Trans_Month == 0.250, "April",
                       ifelse(Trans_Month == 0.333, "May",
                       ifelse(Trans_Month == 0.417, "June",
                       ifelse(Trans_Month == 0.500, "July",
                       ifelse(Trans_Month == 0.583, "August",
                       ifelse(Trans_Month == 0.667, "September",
                       ifelse(Trans_Month == 0.750, "October",
                       ifelse(Trans_Month == 0.833, "November",
                       ifelse(Trans_Month == 0.917, "December",
                       Trans_Month))))))))))))) %>%
  mutate(Trans_Year = floor(Date))
```


```{r}
round(cor(real),2)
pairs(real)
plot(real$Price)
```
**Using the correlation coefficient matrix and the pair-wise matrix to check the linear relationship between the outcome variable and the independent variables.**


```{r}
trainreal<-Real_Estate[1:276,]
testreal<-Real_Estate[277:414,]
```
**We use the first 2/3 of the dataset as the training data and the remaining 1/3 as the test dataset.**


```{r}
library(MASS)
creal = Real_Estate[,2:9]
pcreal<-prcomp(creal)
names(pcreal)
summary(pcreal)
#From the summary, we can see that PC1 already counts for 99.85%, thus it's enough.
round(pcreal$rot[,1],2)
pcrealr<-prcomp(creal,scale=TRUE)
summary(pcrealr)
#After re-scale the modle, we can see that PC5 is the level we want since it has a cumulative proportion of 93.36%.
round(pcrealr$rot[,1],2)
round(pcrealr$rot[,2],2)
t(pcrealr$rot[,2])%*%pcrealr$rot[,1]
#The result for this is 0
```


```{r}
hist(Real_Estate$Age)
hist(Real_Estate$Distance)

hist(Real_Estate$Stores)
hist(sqrt(Real_Estate$Stores))

hist(Real_Estate$Date)
hist(Real_Estate$Latitude)

hist(log(Real_Estate$Longitude))
hist(Real_Estate$Longitude)

hist(Real_Estate$Price)
hist(Real_Estate$Trans_Year)
hist(Real_Estate$Trans_Month)

```
**Based on the graph, we can see that other than Latitude and Price,all other predictors are clearly not normally distributed.Some transformation are necessary for future models.**
```{r}
full_1 = lm(Price~.,data = trainreal)
summary(full_1)
```

```{r}
sigreal<-cov.rob(creal)
mdist<-mahalanobis(creal,center=sigreal$center, cov=sigreal$cov)
n<-dim(creal)[1]
p<-dim(creal)[2]
quant.mdist<-qchisq(1:n/(n+1),p)
```


```{r}
#plot(quant.mdist,sort(mdist)); abline(0,1)
plot(cooks.distance(full_1))
#From the cook's distance and Mahalanobis distance, we can see there is no high-leverage points

```
**Here we use the MASS package to estimate the covariance matrix, calculate the Mahalanobis distance to detect outliers, which pcr is very sensitive to.**
**Use all remaining predictors with the training dataset, and name that model full_1.**


```{r}
summary(full_1)$r.sq
rmse<-function(x,y) sqrt(mean((x-y)^2))
# RMSE for training set and testing set
rmse(fitted(full_1), trainreal$Price)
rmse(predict(full_1,testreal),testreal$Price)
```
**Based on the shape of the plot, we tried 1/Longitude as the transformation and form full_2.**
```{r}
full_2 = lm(Price~Date+Age+Distance+Stores+Latitude+(1/(Longitude))+Trans_Month, data=trainreal)
drop1(full_2,test = 'F')
```
**From this we can see that the Trans_month has the lowest F-value, also, it's a value derived from the transaction date. Thus we can remove it and form model_1.**

```{r}
model_1 = lm(Price~Date+Age+Distance+Stores+Latitude+(1/(Longitude)), data=trainreal)
summary(model_1)
#From the summary, we can see that all predictors are statistically significant. We'll conduct model check.
```
```{r}
plot(model_1)
```
**From Residuals vs Fitted, it's flat, so we have a linear relationship here. Tho there are two small clusters, the overall residuals are roughly equal variance across the range, so the homoscedasticity assumption isn't violated. The sqrt of SR plot also supports this conclusion.**
**The QQ plot looks pretty good for the first part, with many points derived from the end part, so residuals are not normally distributed.**
**From the residual vs Leverage plot, we can see there is no high-influential points.**

```{r}
shapiro.test(model_1$residuals)
#Here, the p-value for the S-W test is way less than 0.05, so we can reject H0 that errors are uncorrelated, and see that the data isn't normally distributed.
```
```{r}
library(car)
durbinWatsonTest(model_1)
#From DW test, we can see that p-value is 0.398 > 0.05. Thus, we failed to reject the H0 that there is no correlation among the residuals.
```
**Now, we'll try to use stepwise selection to find a better model.**
```{r}
step(model_1,data = Real_Estate, direction = 'both')
#From the result, we'll select the one we the least AIC, which is the one using Price ~ Date + Age + Distance + Stores + Latitude + (1/(Longitude)). And this is the same as the model_1 we had previously
```
```{r}
#Now we'll make a normalized data by centering and scaling the data.
normalized_real = as.data.frame(scale(Real_Estate[1:7],center = TRUE, scale = TRUE))
head(normalized_real,10)
```

```{r}
normalized_trainreal<-normalized_real[1:276,]
normalized_testreal<-normalized_real[277:414,]
```
**We use the first 2/3 of the dataset as the training data and the remaining 1/3 as the test dataset.**

```{r}
#Re-fit the model_1 with the new data, and name it as model_2
model_2 = lm(Price ~ Date + Age + Distance + Stores + Latitude + (1/(Longitude)),data = normalized_trainreal)
summary(model_2)
```

```{r}
#Now we'll use the same method to check the new model
plot(model_2)
#The conclusions from the plot is the same as before, there exists a linear relationship, homoscedasticity assumption is supported, residuals aren't normally distributed, and there is no high-influence points.
shapiro.test(model_2$residuals)
#P-value is 0.422 for Shapiro test, meaning we fail to reject the H0 that errors are uncorrelated
durbinWatsonTest(model_2)
#P-value is 0.422 for D-W test, meaning we fail to reject the H0 that there is no correlation among the residuals.
```


```{r}
prediction_1 = predict(model_1,testreal)
error_1 = (mean(testreal$Price-prediction_1))^2
error_1
prediction_2 = predict(model_2,normalized_testreal)
head(prediction_2)
error_2 = (mean(normalized_testreal$Price-prediction_2))^2
error_2
```

```{r}
library(pls)
variable=creal[,-7]

prseat<-prcomp(variable,scale=TRUE)
summary(prseat)
plot(prseat$sdev,type="l",ylab="SD of PC",xlab="PC number")
round(prseat$rot[,1],2)
round(prseat$rot[,2],2)
round(prseat$rot[,3],2)
round(prseat$rot[,4],2)
round(prseat$rot[,5],2)
pcr1<-pcr(Price~.,data=normalized_trainreal)
pcrpred1pos<-predict(pcr1,normalized_trainreal,ncomp=5)
head(pcrpred1pos)

MSE_testingPCR=mean((normalized_testreal$Price - pcrpred1pos) ^ 2)
MSE_test_original=mean((normalized_testreal$Price - predict(model_2, normalized_testreal)) ^ 2)
MSE_testingPCR
MSE_test_original
```

