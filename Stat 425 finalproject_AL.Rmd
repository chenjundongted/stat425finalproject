---
title: "STAT 425 Final Project"
author: "Fall 2021, Ted Chen, "
date: '11/28/21'
output:
  html_document:
    df_print: paged
    theme: readable
    toc: yes
---

## Packages

```{r, message=FALSE}
library(dplyr)
library(tidyverse)
library(ggplot2)
library(stringr)
library(readxl)
library(PerformanceAnalytics)
library(faraway)
library(earth)
library(caret)
set.seed(425)
```

## Exploratory Data Analysis

### Data Overview
Here we import the original file with changed column names. 

We see that there is no missing values. Then, we use the `str()` and `summary()` to get a quick overview of the dataset.
```{r}
real <- read_excel("real_estate.xlsx")
sum(is.null(real))
str(real)
summary(real)
```

From description, the unit used for unit area aren't used in U.S., so we change it first for easier interpretation in the later step. Since the number of the house has no specific meaning, we exclude it from the prediction model.
```{r}
real$Price <- real$Price*0.042*1/3/3*1000
real <- real[,-c(1)]
```

## Data preprocessing

### New Variable `Trans_Year` and `Trans_Month`

We create new column `Trans_Year` and `Trans_Month` with input value derived from the transaction date column, and use that as the 7th predictor. However, we notice an inconsistency between the data description and the actual calculation.
```{r}
Real_Estate = real %>%
  mutate(Trans_Month = round((Date - floor(Date)), digits = 3)) %>%
  mutate(Trans_Month = ifelse(Trans_Month == 0.000, "1",
                       ifelse(Trans_Month == 0.083, "2",
                       ifelse(Trans_Month == 0.167, "3",
                       ifelse(Trans_Month == 0.250, "4",
                       ifelse(Trans_Month == 0.333, "5",
                       ifelse(Trans_Month == 0.417, "6",
                       ifelse(Trans_Month == 0.500, "7",
                       ifelse(Trans_Month == 0.583, "8",
                       ifelse(Trans_Month == 0.667, "9",
                       ifelse(Trans_Month == 0.750, "10",
                       ifelse(Trans_Month == 0.833, "11",
                       ifelse(Trans_Month == 0.917, "12",
                       Trans_Month))))))))))))) %>%
  mutate(Trans_Year = floor(Date)) %>%
  mutate(Date = NULL) %>%
  mutate(Trans_Year = factor(Trans_Year)) %>%
  mutate(Trans_Month = factor(Trans_Month)) %>%
  mutate(Stores = as.integer(Stores))
```

After creating new predictors `Trans_Year` and `Trans_Month` from predictor `Date`, we change all predictors' types to fit the model.Below is the summary of the final data we will use for fitting the model.
```{r}
Real_Estate
```

### Correlation

```{r}
cor(Real_Estate[, -c(7, 8)])
round(cor(Real_Estate[, -c(7, 8)]))
chart.Correlation(Real_Estate[, -c(7, 8)], histogram = TRUE, pch = 10)
```

We notice that only variables `Latitude` and `Price` seem to follow a normal distribution and the highest correlation between individual predictor and response variable is -0.81.


### Collinearity

```{r}
g = lm(Price ~ ., data = Real_Estate[, -c(7, 8)])
x = model.matrix(g)[,-1]
x = x - matrix(apply(x, 2, mean), 414, 5, byrow=TRUE)
x = x / matrix(apply(x, 2, sd), 414, 5, byrow=TRUE)
round(sqrt(vif(x)), dig=2)
```

The standard errors for the coefs are all very small, so we confirm that there is no collinearity issue.

### Training-Testing Dataset

We split the raw dataset to be 70% of training data and 30% of testing data.
```{r}
index = sort(sample(nrow(Real_Estate), nrow(Real_Estate)*0.7))
trainreal = Real_Estate[index,]
testreal = Real_Estate[-index,]
```

## Methods

### Evalutation Metrics
```{r}
calc_training_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

calc_testing_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted)^2))
}

calc_testing_rsquared = function(actual, predicted) {
  rss <- sum((predicted - actual) ^ 2)
  tss <- sum((actual - mean(actual)) ^ 2)
  rsq <- 1 - rss/tss
}

result = data.frame(matrix(ncol = 5, nrow = 0))
```

### Additive Model

```{r}
additive_m = lm(Price ~ ., data = trainreal)
summary(additive_m)
```

From the table, the coefficient of predictor `Trans_Year2013` is NA, which means the variable is not linearly independent. Therefore, we remove this variable from the dataset and re-train the model.

```{r}
trainreal = trainreal[, -c(8)]
testreal = testreal[, -c(8)]
additive_m = lm(Price ~ ., data = trainreal)
s = summary(additive_m)
# Predict using testing data
pred = predict(additive_m, testreal)
r = c("Additive Model", round(calc_training_rmse(additive_m), 4), round(summary(additive_m)$r.squared, 4), round(calc_testing_rmse(testreal$Price, pred), 4), round(calc_testing_rsquared(testreal$Price, pred), 4))
result = rbind(result, r)
colnames(result) = c("Model", "TrainingRMSE", "Training R^2", "TestingRMSE", "Testing R^2")
result
```

### AIC Selection Model

```{r}
AIC_m = step(additive_m, direction = "both", trace = 0)
AIC_m
```

Using the AIC selection, we include the predictors `Age`, `Distance`, `Stores`, `Latitude`, and `Trans_Month` in our model.

```{r}
AIC_selection_m = lm(Price ~ Age + Distance + Stores + Latitude + Trans_Month, data = trainreal)
s = summary(AIC_selection_m)
# Predict using testing data
pred = predict(AIC_selection_m, testreal)
r = c("AIC Model", round(calc_training_rmse(AIC_selection_m), 4), round(summary(AIC_selection_m)$r.squared, 4), round(calc_testing_rmse(testreal$Price, pred), 4), round(calc_testing_rsquared(testreal$Price, pred), 4))
result = rbind(result, r)
result
```

### BIC Selection Model

```{r}
BIC_m = step(additive_m, direction = "both", k = log(length(resid(additive_m))), trace = 0)
BIC_m
```

Using the BIC selection, we include the predictors `Age`, `Distance`, `Stores`, and `Latitude` in our model.

```{r}
BIC_selection_m = lm(Price ~ Age + Distance + Stores + Latitude, data = trainreal)
s = summary(BIC_selection_m)
# Predict using testing data
pred = predict(BIC_selection_m, testreal)
r = c("BIC Model", round(calc_training_rmse(BIC_selection_m), 4), round(summary(BIC_selection_m)$r.squared, 4), round(calc_testing_rmse(testreal$Price, pred), 4), round(calc_testing_rsquared(testreal$Price, pred), 4))
result = rbind(result, r)
result
```

```{r regression diagnostics}

```

### Multivariate Regression Splines Model

```{r}
hyper_grid <- expand.grid(degree = 1:3, 
                          nprune = seq(2, 50, length.out = 10) %>%
                            floor())
```

```{r, message=F, warning=F}
cv_mars <- train(
  x = subset(trainreal, select = -c(Price)),
  y = trainreal$Price,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid, )
```

```{r}
final = cv_mars$results %>%
  filter(nprune == cv_mars$bestTune$nprune, degree == cv_mars$bestTune$degree)
final
r = c("Splines Model", round(final[, 3], 4), round(final[, 4], 4))
ggplot(cv_mars)
```

```{r}
# predict with testing data
pred = predict(cv_mars, testreal)
r = c(r, round(calc_testing_rmse(testreal$Price, pred), 4), round(calc_testing_rsquared(testreal$Price, pred), 4))
result = rbind(result, r)
result
```